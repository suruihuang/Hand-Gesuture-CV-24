{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d7681b-a2e7-4dcc-bd8d-91f64b0a3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/tljh/user/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.utils.data import random_split, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534b34b-aa12-4bfe-ab98-f480ea990977",
   "metadata": {},
   "source": [
    "# Load data \n",
    "Load data from the folders. The output will be a list of tuples, containing the path to image and the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faaa07c-fee6-4be9-ab34-4b89333fc5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-suh222/Hand-Gesuture-CV-24/src/model\n",
      "/home/jupyter-suh222/Hand-Gesuture-CV-24/src/model\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
      "75031\n",
      "('../../asl_data_landmarked/A/landmarked_A1939.jpg', 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# display current directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# get Image path\n",
    "#TRAIN_CLASS_PATH = r'../../asl_alphabet_train/asl_alphabet_train'\n",
    "# train the landmark data\n",
    "TRAIN_CLASS_PATH = r'../../asl_data_landmarked'\n",
    "# TEST_CLASS_PATH = r'../asl_alphabet_test/asl_alphabet_test'\n",
    "\n",
    "#TRAIN_CLASS_PATH = r'..\\data\\asl_alphabet_train\\asl_alphabet_train'\n",
    "#TEST_CLASS_PATH = r'..\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "\n",
    "# hold class labels \n",
    "asl_classes = sorted(os.listdir(TRAIN_CLASS_PATH))\n",
    "trainImages = []\n",
    "image_label_paths = [] # used for customized dataset \n",
    "\n",
    "for i, class_name in enumerate(asl_classes):\n",
    "    class_path = os.path.join(TRAIN_CLASS_PATH, class_name)\n",
    "    fnames = os.listdir(class_path)\n",
    "    for f in fnames:\n",
    "        f_path = os.path.join(TRAIN_CLASS_PATH,class_name, f)\n",
    "        trainImages.append(f_path)\n",
    "        image_label_paths.append((f_path, i))\n",
    "\n",
    "# # display class names \n",
    "print(os.getcwd())\n",
    "print(asl_classes)\n",
    "print(len(image_label_paths))\n",
    "print(image_label_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2c5fb-11b2-4178-b2d4-fe5895477a3c",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5afe98-fa3c-4c46-9c43-cc1f2e6cb26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Dataset ibject using torch \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_label_paths,transform=None):\n",
    "        # store image path\n",
    "        self.image_label_paths = image_label_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the number of total sampels in the dataset\n",
    "        return len(self.image_label_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        imagePath, label = self.image_label_paths[index]\n",
    "        \n",
    "        #image = cv2.imread(imagePath)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.open(imagePath).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            # apply transoformation \n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ImageDataset(LightningModule):\n",
    "    def __init__(self, image_label_paths, batch_size = BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_label_paths = image_label_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        if self.trainDS is not None:\n",
    "            return len(self.trainDS)\n",
    "        elif self.valDS is not None:\n",
    "            return len(self.valDS)\n",
    "        else:\n",
    "            return 0 \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.trainDS is not None:\n",
    "            return self.trainDS[index]\n",
    "        elif self.test_dataset is not None:\n",
    "            return self.test_dataset[index]\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        DS = CustomDataset(self.image_label_paths, self.transform)\n",
    "        # setup training and validation set \n",
    "        DATA_SIZE = len(DS)\n",
    "        TRAIN_SIZE = int(0.8*DATA_SIZE)\n",
    "        VAL_SIZE = DATA_SIZE-TRAIN_SIZE\n",
    "        self.trainDS, self.valDS = random_split(DS, [TRAIN_SIZE, VAL_SIZE])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainDS, batch_size=BATCH_SIZE,shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valDS, batch_size=BATCH_SIZE, shuffle=False) \n",
    "\n",
    "\n",
    "dataset = ImageDataset(image_label_paths)\n",
    "dataset.setup() \n",
    "train_dataloader = dataset.train_dataloader\n",
    "val_dataloader = dataset.val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4c5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, transform=transform, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.root_dir = TRAIN_CLASS_PATH\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_set = datasets.ImageFolder(root=self.root_dir, transform=self.transform)\n",
    "        \n",
    "        n_data = len(dataset)\n",
    "        n_train = int(0.8 * n_data)\n",
    "        n_val = n_data - n_train\n",
    "        train_dataset, val_dataset =  random_split(dataset, [n_train, n_val])\n",
    "\n",
    "        self.train_dataset = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=63)\n",
    "        self.val_dataset = DataLoader(val_dataset, batch_size=self.batch_size,shuffle=False, num_workers=63)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataset\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15288b18",
   "metadata": {},
   "source": [
    "# Setup Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a28d088-0614-4e16-9603-141181134e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # RGB image with 3 channels, output 8 channels with 3x3 kernel \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=8,kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, padding=1)\n",
    "        \n",
    "        # set up first fully connected layers, outchanel * image size \n",
    "        #self.fc1 = nn.Linear(in_features=16*54*54, out_features=120)\n",
    "        self.fc1 = nn.Linear(in_features=128 * 7 * 7, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc4 = nn.Linear(in_features=84, out_features= 20)\n",
    "        self.fc5 = nn.Linear(in_features=20, out_features= len(asl_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply first convolutin layer by ReLu\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        #x = F.dropout(x)\n",
    "        \n",
    "        # second convltn. layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        #x = F.dropout(x)\n",
    "        \n",
    "        # third layer \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        #x = F.dropout(x)\n",
    "        \n",
    "         # fourth layer \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        #x = F.dropout(x)\n",
    "        \n",
    "        # fifth layer \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.dropout(x)\n",
    "        \n",
    "        # flatten\n",
    "        #x = x.view(-1, 16*54*54)\n",
    "        x = x.view(-1, 128*7*7)\n",
    "        #32*28*28\n",
    "        \n",
    "        # activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        # applies the log softmax function\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d43575-882d-4747-97bf-9830bb01e461",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46729a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_dataloader = dataset.test_dataloader\n",
    "datamodule = DataModule()\n",
    "model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e7107-6d00-437e-a1f3-57b78f5c9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | conv1 | Conv2d | 224   \n",
      "1 | conv2 | Conv2d | 1.2 K \n",
      "2 | conv3 | Conv2d | 4.6 K \n",
      "3 | conv4 | Conv2d | 18.5 K\n",
      "4 | conv5 | Conv2d | 73.9 K\n",
      "5 | fc1   | Linear | 1.6 M \n",
      "6 | fc2   | Linear | 30.8 K\n",
      "7 | fc3   | Linear | 10.2 K\n",
      "8 | fc4   | Linear | 1.7 K \n",
      "9 | fc5   | Linear | 609   \n",
      "---------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.990     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb5a47308b4676aa9b160fce911884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=40)\n",
    "trainer.fit(model, datamodule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a3d13d5-1469-4cef-a8f2-5083307bbc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'landmark_model_5_modifiedlayers.pth')\n",
    "#torch.save(model, 'new_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81acdfa-89c9-48a6-b03a-8c02d0841422",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef033ca4-462c-4695-95a0-72d39b162e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-suh222/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /home/jupyter-suh222/Hand-Gesuture-CV-24/src/model/lightning_logs/version_31/checkpoints/epoch=39-step=120080.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from the checkpoint at /home/jupyter-suh222/Hand-Gesuture-CV-24/src/model/lightning_logs/version_31/checkpoints/epoch=39-step=120080.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414934091cf043aa83a294ce73f25291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9168679714202881     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2935883402824402     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9168679714202881    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2935883402824402    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2935883402824402, 'test_acc': 0.9168679714202881}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnn = torch.load(\"model.pth\")\n",
    "val_loader = datamodule.val_dataloader()\n",
    "trainer.test(dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0563f8e5-bda9-4756-90d2-80bf4e88b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model = model.cuda()\n",
    "device = torch.device(\"cuda\")   #\"cuda:0\"\n",
    "\n",
    "model.eval()\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    print(\"evaluating\")\n",
    "    for test_data in datamodule.val_dataloader():\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        #test_images, test_labels = test_data[0], test_data[1]\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a5f671f-a895-4de0-a47a-8fcf48269a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A     0.8473    0.8935    0.8698       385\n",
      "           B     0.8835    0.9560    0.9183       341\n",
      "           C     0.9527    0.9871    0.9696       388\n",
      "           D     0.9036    0.9724    0.9367       434\n",
      "           E     0.8868    0.8501    0.8681       387\n",
      "           F     0.9815    0.9339    0.9571       454\n",
      "           G     0.8780    0.9641    0.9190       418\n",
      "           H     0.9554    0.9122    0.9333       376\n",
      "           I     0.9066    0.9690    0.9368       451\n",
      "           J     0.9794    0.9772    0.9783       438\n",
      "           K     0.9698    0.9246    0.9467       451\n",
      "           L     0.9704    0.9116    0.9401       396\n",
      "           M     0.9524    0.7980    0.8684       401\n",
      "           N     0.8809    0.9112    0.8958       349\n",
      "           O     0.8899    0.9484    0.9182       426\n",
      "           P     0.9972    0.9103    0.9517       390\n",
      "           Q     0.9318    0.9535    0.9425       387\n",
      "           R     0.9171    0.9108    0.9139       437\n",
      "           S     0.8921    0.8411    0.8659       472\n",
      "           T     0.8206    0.9422    0.8772       398\n",
      "           U     0.9134    0.8074    0.8571       431\n",
      "           V     0.8507    0.8806    0.8654       427\n",
      "           W     0.9210    0.8947    0.9077       456\n",
      "           X     0.8759    0.8715    0.8737       397\n",
      "           Y     0.9365    0.9575    0.9469       447\n",
      "           Z     0.9747    0.9747    0.9747       475\n",
      "         del     0.9668    0.9357    0.9510       342\n",
      "     nothing     0.9979    1.0000    0.9990       481\n",
      "       space     0.9118    0.9216    0.9167       370\n",
      "\n",
      "    accuracy                         0.9216     12005\n",
      "   macro avg     0.9223    0.9211    0.9207     12005\n",
      "weighted avg     0.9233    0.9216    0.9215     12005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true,y_pred,target_names=asl_classes,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6746b32-1e4d-4782-9667-10ae82df153d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
