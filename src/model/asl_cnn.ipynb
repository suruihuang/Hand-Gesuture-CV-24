{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d7681b-a2e7-4dcc-bd8d-91f64b0a3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/tljh/user/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms \n",
    "from pytorch_lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534b34b-aa12-4bfe-ab98-f480ea990977",
   "metadata": {},
   "source": [
    "# Load data \n",
    "Load data from the folders. The output will be a list of tuples, containing the path to image and the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faaa07c-fee6-4be9-ab34-4b89333fc5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-suh222/Hand-Gesuture-CV-24/src/model\n",
      "/home/jupyter-suh222/Hand-Gesuture-CV-24/src/model\n",
      "['N', 'D', 'P', 'space', 'Z', 'nothing', 'W', 'I', 'C', 'del', 'Y', 'S', 'G', 'M', 'J', 'T', 'V', 'B', 'H', 'E', 'O', 'Q', 'K', 'A', 'U', 'R', 'X', 'L', 'F']\n",
      "87000\n",
      "('../../asl_alphabet_train/asl_alphabet_train/N/N478.jpg', 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# display current directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# get Image path\n",
    "TRAIN_CLASS_PATH = r'../../asl_alphabet_train/asl_alphabet_train'\n",
    "# TEST_CLASS_PATH = r'../asl_alphabet_test/asl_alphabet_test'\n",
    "\n",
    "# #TRAIN_CLASS_PATH = r'..\\data\\asl_alphabet_train\\asl_alphabet_train'\n",
    "# #TEST_CLASS_PATH = r'..\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "\n",
    "# hold class labels \n",
    "asl_classes = os.listdir(TRAIN_CLASS_PATH)\n",
    "trainImages = []\n",
    "image_label_paths = [] # used for customized dataset \n",
    "\n",
    "for i, class_name in enumerate(asl_classes):\n",
    "    class_path = os.path.join(TRAIN_CLASS_PATH, class_name)\n",
    "    fnames = os.listdir(class_path)\n",
    "    for f in fnames:\n",
    "        f_path = os.path.join(TRAIN_CLASS_PATH,class_name, f)\n",
    "        trainImages.append(f_path)\n",
    "        image_label_paths.append((f_path, i))\n",
    "\n",
    "# # display class names \n",
    "print(os.getcwd())\n",
    "print(asl_classes)\n",
    "print(len(image_label_paths))\n",
    "print(image_label_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2c5fb-11b2-4178-b2d4-fe5895477a3c",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5afe98-fa3c-4c46-9c43-cc1f2e6cb26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Dataset ibject using torch \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_label_paths,transform=None):\n",
    "        # store image path\n",
    "        self.image_label_paths = image_label_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the number of total sampels in the dataset\n",
    "        return len(self.image_label_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        imagePath, label = self.image_label_paths[index]\n",
    "        \n",
    "        #image = cv2.imread(imagePath)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.open(imagePath).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            # apply transoformation \n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ImageDataset(LightningModule):\n",
    "    def __init__(self, image_label_paths, batch_size = BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_label_paths = image_label_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        if self.trainDS is not None:\n",
    "            return len(self.trainDS)\n",
    "        elif self.valDS is not None:\n",
    "            return len(self.valDS)\n",
    "        else:\n",
    "            return 0 \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.trainDS is not None:\n",
    "            return self.trainDS[index]\n",
    "        elif self.test_dataset is not None:\n",
    "            return self.test_dataset[index]\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        DS = CustomDataset(self.image_label_paths, self.transform)\n",
    "        # setup training and validation set \n",
    "        DATA_SIZE = len(DS)\n",
    "        TRAIN_SIZE = int(0.8*DATA_SIZE)\n",
    "        VAL_SIZE = DATA_SIZE-TRAIN_SIZE\n",
    "        self.trainDS, self.valDS = random_split(DS, [TRAIN_SIZE, VAL_SIZE])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainDS, batch_size=BATCH_SIZE,shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valDS, batch_size=BATCH_SIZE, shuffle=False) \n",
    "\n",
    "\n",
    "dataset = ImageDataset(image_label_paths)\n",
    "dataset.setup() \n",
    "train_dataloader = dataset.train_dataloader\n",
    "val_dataloader = dataset.val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4c5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, transform=transform, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.root_dir = TRAIN_CLASS_PATH\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data_set = datasets.ImageFolder(root=self.root_dir, transform=self.transform)\n",
    "        \n",
    "        n_data = len(dataset)\n",
    "        n_train = int(0.8 * n_data)\n",
    "        n_val = n_data - n_train\n",
    "        train_dataset, val_dataset =  random_split(dataset, [n_train, n_val])\n",
    "\n",
    "        self.train_dataset = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dataset = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataset\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15288b18",
   "metadata": {},
   "source": [
    "# Setup Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a28d088-0614-4e16-9603-141181134e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # RGB image with 3 channels, output 6 channels with 3x3 kernel \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=3)\n",
    "        \n",
    "        # set up first fully connected layers, outchanel * image size \n",
    "        self.fc1 = nn.Linear(in_features=12*54*54, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=20)\n",
    "        self.fc4 = nn.Linear(in_features=20, out_features= len(asl_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply first convolutin layer by ReLu\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        # second convltn. layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(-1, 12*54*54)\n",
    "        \n",
    "        # activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # applies the log softmax function\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = y_hat.argmax(dim=1, keepdim=True)\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / y.shape[0]\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d43575-882d-4747-97bf-9830bb01e461",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46729a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_dataloader = dataset.test_dataloader\n",
    "datamodule = DataModule()\n",
    "model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2e7107-6d00-437e-a1f3-57b78f5c9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | conv1 | Conv2d | 168   \n",
      "1 | conv2 | Conv2d | 660   \n",
      "2 | fc1   | Linear | 4.2 M \n",
      "3 | fc2   | Linear | 10.2 K\n",
      "4 | fc3   | Linear | 1.7 K \n",
      "5 | fc4   | Linear | 609   \n",
      "---------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.850    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-suh222/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/home/jupyter-suh222/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3543ad895f49c09b4f57d20423c77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, datamodule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3d13d5-1469-4cef-a8f2-5083307bbc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'new_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81acdfa-89c9-48a6-b03a-8c02d0841422",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef033ca4-462c-4695-95a0-72d39b162e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-suh222/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /home/jupyter-suh222/Hand-Gesuture-CV-24/src/model/lightning_logs/version_9/checkpoints/epoch=9-step=34800.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from the checkpoint at /home/jupyter-suh222/Hand-Gesuture-CV-24/src/model/lightning_logs/version_9/checkpoints/epoch=9-step=34800.ckpt\n",
      "/home/jupyter-suh222/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f752f6d599a6433a972ebc8f73c0ec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9617816209793091     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1253911256790161     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9617816209793091    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1253911256790161    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.1253911256790161, 'test_acc': 0.9617816209793091}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cnn = torch.load(\"model.pth\")\n",
    "val_loader = datamodule.val_dataloader()\n",
    "trainer.test(dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0563f8e5-bda9-4756-90d2-80bf4e88b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mappend(test_labels[i]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     16\u001b[0m             y_pred\u001b[38;5;241m.\u001b[39mappend(pred[i]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true,y_pred,target_names\u001b[38;5;241m=\u001b[39m\u001b[43mclass_names\u001b[49m,digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "model = model.cuda()\n",
    "device = torch.device(\"cuda\")   #\"cuda:0\"\n",
    "\n",
    "model.eval()\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    print(\"evaluating\")\n",
    "    for test_data in datamodule.val_dataloader():\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        #test_images, test_labels = test_data[0], test_data[1]\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a5f671f-a895-4de0-a47a-8fcf48269a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N     1.0000    0.9959    0.9979       483\n",
      "           D     0.9798    0.9878    0.9838       491\n",
      "           P     0.9644    0.9935    0.9788       464\n",
      "       space     0.9894    0.9852    0.9873       474\n",
      "           Z     0.9939    0.9878    0.9908       491\n",
      "     nothing     0.9939    1.0000    0.9970       491\n",
      "           W     0.9615    0.9855    0.9734       482\n",
      "           I     0.9721    0.9939    0.9828       490\n",
      "           C     0.9980    0.9749    0.9863       517\n",
      "         del     0.9823    0.9960    0.9891       501\n",
      "           Y     0.9835    0.8948    0.9371       466\n",
      "           S     0.9571    0.9859    0.9713       498\n",
      "           G     0.9471    0.9598    0.9534       448\n",
      "           M     0.9778    0.9898    0.9837       489\n",
      "           J     0.9855    0.9896    0.9876       481\n",
      "           T     0.9958    0.9715    0.9835       492\n",
      "           V     0.9734    0.9615    0.9674       494\n",
      "           B     0.9765    0.9881    0.9822       504\n",
      "           H     0.9664    0.9558    0.9611       452\n",
      "           E     0.9706    0.9747    0.9727       475\n",
      "           O     0.9977    0.9911    0.9944       447\n",
      "           Q     0.9979    0.9691    0.9833       486\n",
      "           K     0.9819    0.9665    0.9741       448\n",
      "           A     0.9638    0.9660    0.9649       441\n",
      "           U     0.9570    0.9859    0.9713       497\n",
      "           R     1.0000    0.9579    0.9785       499\n",
      "           X     0.9362    0.9913    0.9630       459\n",
      "           L     0.9959    0.9897    0.9928       486\n",
      "           F     0.9853    0.9873    0.9863       474\n",
      "\n",
      "    accuracy                         0.9787     13920\n",
      "   macro avg     0.9788    0.9785    0.9785     13920\n",
      "weighted avg     0.9790    0.9787    0.9787     13920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true,y_pred,target_names=asl_classes,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6746b32-1e4d-4782-9667-10ae82df153d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
